{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '0.py', '2.csv', '3.csv', '3chTest', '3chTest1', '3chTrain', '3chTrain1', '3chTrain2', '4.csv', '5.csv', 'a.jpg', 'batches.meta', 'cDCGAN_VS_CNN-Copy1.ipynb', 'cDCGAN_VS_CNN.ipynb', 'checkpoint', 'data', 'data_batch_1', 'data_batch_2', 'data_batch_3', 'data_batch_4', 'data_batch_5', 'imageChangeChannel.ipynb', 'readme.html', 'resize.py', 'splite_data.py', 'SSGAN_VS_CNN_calculate_confusion_matrix.ipynb', 'test', 'test.png', 'test1.png', 'test2', 'test2.png', 'test3', 'testNew', 'test_batch', 'train', 'train2', 'train3', 'trainNew', 'Untitled-Copy1.ipynb', 'Untitled1.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import slim\n",
    "CIFAR_DIR = \"../cifar-10-batches-py\"\n",
    "print(os.listdir(CIFAR_DIR))\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "def load_Traindata():\n",
    "    train = []\n",
    "    file_names = os.listdir('./3chTrain1/')\n",
    "    for i in range(len(file_names)):\n",
    "        im = Image.open('./3chTrain1/'+file_names[i])\n",
    "        im2 = np.array(im)\n",
    "        train.append(im2)\n",
    "    train2 = np.array(train)\n",
    "    train_lable = np.zeros((len(train2),1),dtype='int8')\n",
    "    for i in range(6):\n",
    "        for j in range(i*1711, (i+1)*1711):\n",
    "            train_lable[j,0] = i\n",
    "   \n",
    "    train_lable = train_lable.reshape((train_lable.shape[0],))\n",
    "    return (train2, train_lable)\n",
    "def load_Testdata():\n",
    "    test = []\n",
    "    file_names = os.listdir('./3chTest1/')\n",
    "    for i in range(len(file_names)):\n",
    "        im = Image.open('./3chTest1/'+file_names[i])\n",
    "        im2 = np.array(im)\n",
    "        test.append(im2)\n",
    "    test2 = np.array(test)\n",
    "    test_lable = np.zeros((len(test2),1),dtype='int8')\n",
    "    for i in range(6):\n",
    "        for j in range(i*435, (i+1)*435):\n",
    "            test_lable[j,0] = i\n",
    "    test_lable = test_lable.reshape((test_lable.shape[0],))\n",
    "    return (test2, test_lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(checkpoint_dir, saver, sess):\n",
    "    import re\n",
    "    print(\" [*] Reading checkpoints...\")\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "        counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\", ckpt_name)).group(0))\n",
    "        print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "        return True, counter\n",
    "    else:\n",
    "        print(\" [*] Failed to find a checkpoint\")\n",
    "        return False, 0\n",
    "\n",
    "\n",
    "def save(checkpoint_dir, step, saver, sess):\n",
    "    model_name = \"cDCGAN.model\"\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver.save(sess, os.path.join(checkpoint_dir, model_name), global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = 0\n",
    "path = 'D:/cifar-10-batches-py/3chTest1'\n",
    "\n",
    "for lists in os.listdir(path):\n",
    "    sub_path = os.path.join(path, lists)\n",
    "    if os.path.isfile(sub_path):\n",
    "        data_len = data_len+1\n",
    "class SteelData:\n",
    "    def __init__(self,  need_shuffle, isTrain):\n",
    "        all_data = []\n",
    "        all_labels = []\n",
    "        if isTrain:\n",
    "            data, labels = load_Traindata()\n",
    "        else:\n",
    "            data, labels = load_Testdata()\n",
    "        all_data.append(data)\n",
    "        all_labels.append(labels)\n",
    "        self._data = np.vstack(all_data)\n",
    "        self._data = self._data / 127.5 - 1\n",
    "        self._labels = np.hstack(all_labels)\n",
    "        \n",
    "        self._num_examples = self._data.shape[0]\n",
    "        self._need_shuffle = need_shuffle\n",
    "        self._indicator = 0\n",
    "        if self._need_shuffle:\n",
    "            self._shuffle_data()\n",
    "            \n",
    "    def _shuffle_data(self):\n",
    "        # [0,1,2,3,4,5] -> [5,3,2,4,0,1]\n",
    "        p = np.random.permutation(self._num_examples)\n",
    "        self._data = self._data[p]\n",
    "        self._labels = self._labels[p]\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"return batch_size examples as a batch.\"\"\"\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self._num_examples:\n",
    "            if self._need_shuffle:\n",
    "                self._shuffle_data()\n",
    "                self._indicator = 0\n",
    "                end_indicator = batch_size\n",
    "            else:\n",
    "                raise Exception(\"have no more examples\")\n",
    "        if end_indicator > self._num_examples:\n",
    "            raise Exception(\"batch size is larger than all examples\")\n",
    "        batch_data = self._data[self._indicator: end_indicator]\n",
    "        batch_labels = self._labels[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    def get_test_batch(self):\n",
    "        \"\"\"return batch_size examples as a batch.\"\"\"\n",
    "        for i in range(6):\n",
    "            # 训练集打乱顺序随机输入\n",
    "            data_len_per_class = data_len//6\n",
    "            order = np.random.permutation(data_len//6)           \n",
    "            if i == 0:\n",
    "                for k in range(60):\n",
    "                    if k == 0:\n",
    "                        batch_data = self._data[(i*data_len_per_class+order[0])]\n",
    "                        batch_data = np.reshape(batch_data, (1, 64, 64, 3))\n",
    "                        batch_labels = self._labels[(i*data_len_per_class+order[0])]\n",
    "                    else:\n",
    "                        batch_data = np.concatenate((batch_data,np.reshape(self._data[(i*data_len_per_class+order[k])], (1, 64, 64, 3))), axis=0)\n",
    "                        batch_labels = np.append(batch_labels,self._labels[(i*data_len_per_class+order[0])])\n",
    "            else:\n",
    "                for k in range(60):\n",
    "                        batch_data = np.concatenate((batch_data,np.reshape(self._data[(i*data_len_per_class+order[k])], (1, 64, 64, 3))), axis=0)\n",
    "                        batch_labels = np.append(batch_labels,self._labels[(i*data_len_per_class+order[k])])\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "train_data = SteelData(True, True)\n",
    "test_data = SteelData(False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoint/train_cDCGAN\\cDCGAN.model-14\n",
      " [*] Success to read cDCGAN.model-14\n",
      " [*] Load SUCCESS\n",
      "[[56  0  3  1  0  0]\n",
      " [ 0 35  6  8  1 10]\n",
      " [ 0  0 60  0  0  0]\n",
      " [ 1 17  0 42  0  0]\n",
      " [ 0  2  0  0 58  0]\n",
      " [ 0  1  0  0  0 59]]\n",
      "[Test ] Step: 1, acc: 0.86500\n",
      "[[56  0  2  1  1  0]\n",
      " [ 0 37  6 12  0  5]\n",
      " [ 0  0 59  0  0  1]\n",
      " [ 0 16  0 44  0  0]\n",
      " [ 0  2  0  0 58  0]\n",
      " [ 0  0  0  0  0 60]]\n",
      "[Test ] Step: 2, acc: 0.87083\n",
      "[[55  0  2  0  3  0]\n",
      " [ 0 29  6 13  3  9]\n",
      " [ 0  0 59  0  0  1]\n",
      " [ 2 21  0 37  0  0]\n",
      " [ 0  1  0  0 59  0]\n",
      " [ 0  1  1  0  0 58]]\n",
      "[Test ] Step: 3, acc: 0.86639\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, 64, 64, 3])\n",
    "#x = tf.placeholder(tf.float32, [None, 3072])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "# [None], eg: [0,5,6,3]\n",
    "#x_image = tf.reshape(x, [-1, 3, 32, 32])\n",
    "# 32*32\n",
    "#x_image = tf.transpose(x_image, perm=[0, 2, 3, 1])\n",
    "\n",
    "with tf.variable_scope('cDCGAN', reuse=tf.AUTO_REUSE):\n",
    "    w_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    cnum = 64\n",
    "\n",
    "    conv1 = tf.layers.conv2d(x, cnum, 5, 2, padding='same', kernel_initializer=w_init,\n",
    "                                         bias_initializer=b_init, activation=tf.nn.leaky_relu, name='conv1')\n",
    "    bn1 = tf.layers.batch_normalization(conv1)\n",
    "    conv2 = tf.layers.conv2d(bn1, 2 * cnum, 3, 2, padding='same', kernel_initializer=w_init,\n",
    "                                         bias_initializer=b_init, activation=tf.nn.leaky_relu, name='conv2')\n",
    "    bn2 = tf.layers.batch_normalization(conv2)\n",
    "    conv3 = tf.layers.conv2d(bn2, 4 * cnum, 3, 2, padding='same', kernel_initializer=w_init,\n",
    "                                         bias_initializer=b_init, activation=tf.nn.leaky_relu, name='conv3')\n",
    "    bn3 = tf.layers.batch_normalization(conv3)\n",
    "    conv4 = tf.layers.conv2d(bn3, 8 * cnum, 3, 2, padding='same', kernel_initializer=w_init,\n",
    "                                         bias_initializer=b_init, activation=tf.nn.leaky_relu, name='conv4')\n",
    "    # output real or false\n",
    "    fla = tf.layers.flatten(conv4, name='flatten')\n",
    "    fc1 = tf.layers.dense(fla, 1024, name='fc1')\n",
    "\n",
    "\n",
    "                # output cls\n",
    "                # 共享fc1层\n",
    "    y_ = tf.layers.dense(fc1, 6, name='cls')\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
    "# y_ -> sofmax\n",
    "# y -> one_hot\n",
    "# loss = ylogy_\n",
    "\n",
    "# indices\n",
    "predict = tf.argmax(y_, 1)\n",
    "# [1,0,1,1,1,0,0,0]\n",
    "correct_prediction = tf.equal(predict, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = 36\n",
    "train_steps = 1000\n",
    "test_steps = 10\n",
    "\n",
    "# train 10k: 71.35%\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     all_steps = []\n",
    "#     all_train_acc = []\n",
    "#     all_test_steps = []\n",
    "#     all_test_acc = []\n",
    "#     all_test_acc_val = []\n",
    "#     all_loss = []\n",
    "#     saver = tf.train.Saver()\n",
    "#     #show_all_variables()\n",
    "#     # log dir\n",
    "#     #summary_op = tf.summary.merge_all()\n",
    "\n",
    "#     #writer = tf.summary.FileWriter(\"./logs/\" + '_'.join(\n",
    "#         #[config.DATASET_TRAIN, config.MODEL_NAME]), sess.graph)\n",
    "\n",
    "#     log_prefix = './checkpoint' + '/' + 'train_cDCGAN'\n",
    "#     counter = 1\n",
    "#     #start_time = time.time()\n",
    "#     could_load, checkpoint_counter = load(log_prefix, saver, sess)\n",
    "#     if could_load:\n",
    "#         counter = checkpoint_counter\n",
    "#         print(\" [*] Load SUCCESS\")\n",
    "#     else:\n",
    "#         print(\" [!] Load failed...\")\n",
    "#     for i in range(train_steps):\n",
    "        \n",
    "        \n",
    "        \n",
    "#         batch_data, batch_labels = train_data.next_batch(batch_size)\n",
    "#         loss_val, acc_val, _ = sess.run(\n",
    "#             [loss, accuracy, train_op],\n",
    "#             feed_dict={\n",
    "#                 x: batch_data,\n",
    "#                 y: batch_labels})\n",
    "#         if (i+1) % 50 == 0:\n",
    "#             save(log_prefix, counter, saver, sess)\n",
    "#             print('[Train] Step: %d, loss: %4.5f, acc: %4.5f' \n",
    "#                   % (i+1, loss_val, acc_val))\n",
    "#             all_steps.append(i+1)\n",
    "#             all_train_acc.append(acc_val)\n",
    "#             all_loss.append(loss_val)\n",
    "#             #if (i+1) % 1000 == 0:\n",
    "#             test_data = SteelData(False,False)\n",
    "\n",
    "\n",
    "#             test_batch_data, test_batch_labels \\\n",
    "#                         = test_data.next_batch(1)\n",
    "#             test_acc_val = sess.run(\n",
    "#                         [accuracy],\n",
    "#                         feed_dict = {\n",
    "#                             x: test_batch_data, \n",
    "#                             y: test_batch_labels\n",
    "#                         })\n",
    "#             all_test_acc_val.append(test_acc_val)\n",
    "\n",
    "#             all_test_steps.append(i)\n",
    "#             #test_acc = np.mean(all_test_acc_val)\n",
    "#             test_acc =  np.mean(test_acc_val)\n",
    "#         #all_test_acc.append(test_acc)\n",
    "#             print('[Test ] Step: %d, acc: %4.5f'\n",
    "#                   % (i+1, test_acc))\n",
    "#             counter += 1\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver = tf.train.Saver()\n",
    "    log_prefix = './checkpoint' + '/' + 'train_cDCGAN'\n",
    "    counter = 1\n",
    "        #start_time = time.time()\n",
    "    could_load, checkpoint_counter = load(log_prefix, saver, sess)\n",
    "    if could_load:\n",
    "        counter = checkpoint_counter\n",
    "        print(\" [*] Load SUCCESS\")\n",
    "    else:\n",
    "        print(\" [!] Load failed...\")\n",
    "      \n",
    "    for i in range(train_steps):\n",
    "#         batch_data, batch_labels = train_data.next_batch(batch_size)\n",
    "#         loss_val, acc_val, _ = sess.run(\n",
    "#             [loss, accuracy, train_op],\n",
    "#             feed_dict={\n",
    "#                 x: batch_data,\n",
    "#                 y: batch_labels})\n",
    "#         if (i+1) % 100 == 0:\n",
    "#             print('[Train] Step: %d, loss: %4.5f, acc: %4.5f' \n",
    "#                   % (i+1, loss_val, acc_val))\n",
    "        \n",
    "        test_data = SteelData(False, False)\n",
    "        all_test_acc_val = []\n",
    "        for j in range(test_steps):\n",
    "            test_batch_data, test_batch_labels \\\n",
    "                    = test_data.get_test_batch()\n",
    "            test_acc_val = sess.run(\n",
    "                    [accuracy],\n",
    "                    feed_dict = {\n",
    "                        x: test_batch_data, \n",
    "                        y: test_batch_labels\n",
    "                    })\n",
    "            all_test_acc_val.append(test_acc_val)\n",
    "        test_acc = np.mean(all_test_acc_val)\n",
    "        confusion_matrix = tf.confusion_matrix(predictions=predict,\n",
    "                                               labels=test_batch_labels,\n",
    "                                               num_classes=6, dtype=tf.int32,\n",
    "                                               name=None, weights=None)\n",
    "        confusion_matrix1 = sess.run(confusion_matrix, feed_dict={\n",
    "                        x: test_batch_data, \n",
    "                        y: test_batch_labels\n",
    "                    })\n",
    "        print(confusion_matrix1)\n",
    "        print('[Test ] Step: %d, acc: %4.5f'\n",
    "                  % (i+1, test_acc))\n",
    "        \n",
    "    all_test_acc_val = []\n",
    "     \n",
    "    #print(all_steps)      \n",
    "    #print(all_train_acc)\n",
    "#     fig,ax = plt.subplots()\n",
    " \n",
    "#     #plt.xlabel('migration speed (MB/s)')\n",
    "#     #plt.ylabel('migration time (s); request delay (ms)')\n",
    "\n",
    "#     \"\"\"set interval for y label\"\"\"\n",
    "#     #yticks = range(0,1,11)\n",
    "#     #ax.set_yticks(yticks)\n",
    "\n",
    "\n",
    "#     \"\"\"set min and max value for axes\"\"\"\n",
    "#     #ax.set_ylim([0,1])\n",
    "#     ax.set_xlim([0,10000])\n",
    "\n",
    "\n",
    "#     #x = [57,56,55,54,53,52,51,50,49,48,47,46,45,44,43]\n",
    "#     line1, = plt.plot(all_steps,all_train_acc,label=\"train accrucy\")\n",
    "#     line2, =plt.plot(all_steps,all_test_acc_val,'-.',label=\"test accrucy\")\n",
    "#     line3, =plt.plot(all_steps,all_loss,'--',label=\"test accrucy\")\n",
    "#     #plt.plot(all_steps,all_test_acc_val,\"+-\",label=\"request delay\")\n",
    "\n",
    "#     \"\"\"open the grid\"\"\"\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     #plt.legend(bbox_to_anchor=(1.0, 1), loc=1, borderaxespad=0.)\n",
    "#     plt.legend([line1, line2,line3], ['train accrucy', 'test accrucy','loss'], loc = 'best') \n",
    "#     #plt.savefig('./test.png')\n",
    "#     plt.savefig('./test1.png')\n",
    "#     plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8888888888888888], [0.7222222222222222], [0.9444444444444444], [0.8333333333333334], [0.8611111111111112], [0.9166666666666666], [0.8055555555555556], [0.8888888888888888], [0.8333333333333334], [0.8888888888888888]]\n"
     ]
    }
   ],
   "source": [
    "print(all_test_acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    " \n",
    "#plt.xlabel('migration speed (MB/s)')\n",
    "    #plt.ylabel('migration time (s); request delay (ms)')\n",
    "\n",
    "    #\"\"\"set interval for y label\"\"\"\n",
    "    #yticks = range(0,1,11)\n",
    "    #ax.set_yticks(yticks)\n",
    "\n",
    "\n",
    "    #\"\"\"set min and max value for axes\"\"\"\n",
    "ax.set_ylim([0,1.1])\n",
    "ax.set_xlim([0,10000])\n",
    "\n",
    "\n",
    "    #x = [57,56,55,54,53,52,51,50,49,48,47,46,45,44,43]\n",
    "line1, = plt.plot(all_steps,all_train_acc,label=\"train accrucy\")\n",
    "line2, =plt.plot(all_steps,all_test_acc_val,'-.',label=\"test accrucy\")\n",
    "    #plt.plot(all_steps,all_test_acc_val,\"+-\",label=\"request delay\")\n",
    "\n",
    " #\"\"\"open the grid\"\"\"\n",
    "plt.grid(True)\n",
    "\n",
    "    #plt.legend(bbox_to_anchor=(1.0, 1), loc=1, borderaxespad=0.)\n",
    "plt.legend([line1, line2], ['train accrucy', 'test accrucy'], loc = 'best') \n",
    "plt.savefig('./test.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    " \n",
    "#plt.xlabel('migration speed (MB/s)')\n",
    "    #plt.ylabel('migration time (s); request delay (ms)')\n",
    "\n",
    "    #\"\"\"set interval for y label\"\"\"\n",
    "    #yticks = range(0,1,11)\n",
    "    #ax.set_yticks(yticks)\n",
    "\n",
    "\n",
    "    #\"\"\"set min and max value for axes\"\"\"\n",
    "#ax.set_ylim([0,1.1])\n",
    "ax.set_xlim([0,10000])\n",
    "\n",
    "\n",
    "    #x = [57,56,55,54,53,52,51,50,49,48,47,46,45,44,43]\n",
    "#line1, = plt.plot(all_steps,all_train_acc,label=\"train accrucy\")\n",
    "#line2, =plt.plot(all_steps,all_test_acc_val,'-.',label=\"test accrucy\")\n",
    "    #plt.plot(all_steps,all_test_acc_val,\"+-\",label=\"request delay\")\n",
    "\n",
    " #\"\"\"open the grid\"\"\"\n",
    "plt.grid(True)\n",
    "plt.plot(all_steps,all_loss,'--',label=\"test accrucy\")\n",
    "    #plt.legend(bbox_to_anchor=(1.0, 1), loc=1, borderaxespad=0.)\n",
    "#plt.legend([line1, line2], ['train accrucy', 'test accrucy'], loc = 'best') \n",
    "plt.savefig('./test2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3333333333333333, 0.5, 0.4722222222222222, 0.4722222222222222, 0.5833333333333334, 0.8333333333333334, 0.7222222222222222, 0.75, 0.7222222222222222, 0.7777777777777778, 0.8888888888888888, 0.9444444444444444, 0.9722222222222222, 0.8888888888888888, 0.8888888888888888, 0.9444444444444444, 0.9166666666666666, 0.8333333333333334, 0.8888888888888888, 1.0, 0.8333333333333334, 1.0, 0.9444444444444444, 0.8333333333333334, 0.9166666666666666, 0.9444444444444444, 0.8611111111111112, 0.9166666666666666, 1.0, 1.0, 0.9722222222222222, 0.8888888888888888, 0.8888888888888888, 0.8611111111111112, 0.8611111111111112, 1.0, 0.9722222222222222, 0.9166666666666666, 0.8611111111111112, 0.9722222222222222, 0.9166666666666666, 0.8888888888888888, 0.9722222222222222, 1.0, 0.9166666666666666, 0.9722222222222222, 1.0, 0.9722222222222222, 0.9722222222222222, 0.9444444444444444, 0.8611111111111112, 0.9444444444444444, 1.0, 0.9722222222222222, 1.0, 0.8888888888888888, 0.9444444444444444, 0.9444444444444444, 0.9166666666666666, 1.0, 1.0, 1.0, 0.9722222222222222, 1.0, 0.9444444444444444, 0.9444444444444444, 0.9722222222222222, 1.0, 0.9722222222222222, 1.0, 0.9444444444444444, 0.7777777777777778, 0.8055555555555556, 0.9722222222222222, 0.8888888888888888, 0.8888888888888888, 0.9722222222222222, 0.9444444444444444, 0.9722222222222222, 1.0, 0.9722222222222222, 1.0, 1.0, 1.0, 0.9444444444444444, 0.9722222222222222, 1.0, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9444444444444444, 0.9166666666666666, 0.7777777777777778, 0.8888888888888888, 1.0, 0.9722222222222222, 1.0, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 1.0, 1.0, 0.9722222222222222, 0.9722222222222222, 0.9722222222222222, 1.0, 0.8888888888888888, 0.9166666666666666, 1.0, 0.9722222222222222, 0.9444444444444444, 1.0, 1.0, 1.0, 0.9722222222222222, 1.0, 0.9444444444444444, 1.0, 1.0, 0.9722222222222222, 0.9722222222222222, 0.9444444444444444, 1.0, 1.0, 0.9444444444444444, 1.0, 1.0, 1.0, 1.0, 0.9722222222222222, 0.9722222222222222, 0.9166666666666666, 0.9166666666666666, 0.9722222222222222, 0.9444444444444444, 0.9722222222222222, 1.0, 0.9444444444444444, 0.9722222222222222, 1.0, 1.0, 0.9444444444444444, 1.0, 1.0, 1.0, 1.0, 0.9444444444444444, 1.0, 0.9722222222222222, 1.0, 0.9722222222222222, 0.9722222222222222, 1.0, 0.9444444444444444, 1.0, 1.0, 0.9444444444444444, 1.0, 0.9444444444444444, 0.9444444444444444, 1.0, 0.9722222222222222, 0.9722222222222222, 1.0, 0.9722222222222222, 1.0, 0.9444444444444444, 0.9722222222222222, 0.9722222222222222, 1.0, 1.0, 1.0, 0.9444444444444444, 1.0, 0.9722222222222222, 0.9444444444444444, 0.9722222222222222, 0.9444444444444444, 1.0, 0.9722222222222222, 0.9722222222222222, 0.9722222222222222, 1.0, 1.0, 0.9722222222222222, 1.0, 1.0, 1.0, 1.0, 0.9722222222222222, 0.9722222222222222, 1.0]\n"
     ]
    }
   ],
   "source": [
    "row1 = all_train_acc\n",
    "print(row1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "row2 = all_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(map(lambda x:[x],row1))\n",
    "with open('4.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    for i in data:\n",
    "            f_csv.writerow(i)\n",
    "    #f_csv.writerows(row2)\n",
    "    #f_csv.writerows(row1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(all_test_acc_val[-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(map(lambda x:[x],row2))\n",
    "with open('5.csv','w') as f:\n",
    "    f_csv = csv.writer(f)\n",
    "    for i in data:\n",
    "            f_csv.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, 64, 64, 3])\n",
    "#x = tf.placeholder(tf.float32, [None, 3072])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "\n",
    "with tf.variable_scope('cDCGAN', reuse=tf.AUTO_REUSE):\n",
    "    w_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "    b_init = tf.constant_initializer(0.0)\n",
    "    cnum = 64\n",
    "\n",
    "    conv1 = tf.layers.conv2d(x, cnum, 5, 2, padding='same', kernel_initializer=w_init,\n",
    "                                         bias_initializer=b_init, activation=tf.nn.leaky_relu, name='conv1')\n",
    "    bn1 = tf.layers.batch_normalization(conv1)\n",
    "    conv2 = tf.layers.conv2d(bn1, 2 * cnum, 3, 2, padding='same', kernel_initializer=w_init,\n",
    "                                         bias_initializer=b_init, activation=tf.nn.leaky_relu, name='conv2')\n",
    "    bn2 = tf.layers.batch_normalization(conv2)\n",
    "    conv3 = tf.layers.conv2d(bn2, 4 * cnum, 3, 2, padding='same', kernel_initializer=w_init,\n",
    "                                         bias_initializer=b_init, activation=tf.nn.leaky_relu, name='conv3')\n",
    "    bn3 = tf.layers.batch_normalization(conv3)\n",
    "    conv4 = tf.layers.conv2d(bn3, 8 * cnum, 3, 2, padding='same', kernel_initializer=w_init,\n",
    "                                         bias_initializer=b_init, activation=tf.nn.leaky_relu, name='conv4')\n",
    "    # output real or false\n",
    "    fla = tf.layers.flatten(conv4, name='flatten')\n",
    "    fc1 = tf.layers.dense(fla, 1024, name='fc1')\n",
    "\n",
    "\n",
    "                # output cls\n",
    "                # 共享fc1层\n",
    "    y_ = tf.layers.dense(fc1, 6, name='cls')\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)\n",
    "# y_ -> sofmax\n",
    "# y -> one_hot\n",
    "# loss = ylogy_\n",
    "\n",
    "# indices\n",
    "predict = tf.argmax(y_, 1)\n",
    "# [1,0,1,1,1,0,0,0]\n",
    "correct_prediction = tf.equal(predict, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "\n",
    "with tf.name_scope('train_op'):\n",
    "    train_op = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = 5\n",
    "train_steps = 10000\n",
    "test_steps = 100\n",
    "\n",
    "train 10k: 71.35%\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    all_steps = []\n",
    "    all_train_acc = []\n",
    "    all_test_steps = []\n",
    "    all_test_acc = []\n",
    "    all_test_acc_val = []\n",
    "    all_loss = []\n",
    "    saver = tf.train.Saver()\n",
    "    #show_all_variables()\n",
    "    # log dir\n",
    "    #summary_op = tf.summary.merge_all()\n",
    "\n",
    "    #writer = tf.summary.FileWriter(\"./logs/\" + '_'.join(\n",
    "        #[config.DATASET_TRAIN, config.MODEL_NAME]), sess.graph)\n",
    "\n",
    "    log_prefix = './checkpoint' + '/' + 'train_cDCGAN'\n",
    "    counter = 1\n",
    "    #start_time = time.time()\n",
    "    could_load, checkpoint_counter = load(log_prefix, saver, sess)\n",
    "    if could_load:\n",
    "        counter = checkpoint_counter\n",
    "        print(\" [*] Load SUCCESS\")\n",
    "    else:\n",
    "        print(\" [!] Load failed...\")\n",
    "    for i in range(train_steps):\n",
    "        i = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        order = np.random.permutation(data_len)\n",
    "        for idx in range(5000):\n",
    "            batch_data_files = []\n",
    "            batch_label = []\n",
    "            batch_data_files.append(data_list[order[idx]])\n",
    "            batch_label.append(label_list[order[idx]])\n",
    "            #batch_data_files.append(data_list[idx])\n",
    "            #batch_label.append(label_list[idx])\n",
    "            batch_data = [\n",
    "                get_image(batch_file, grayscale=True) for batch_file in batch_data_files]\n",
    "            batch_images = np.array(batch_data).astype(np.float32)\n",
    "            batch_images = np.expand_dims(batch_images, axis=3)\n",
    "            batch_label = np.array(batch_label).astype(np.uint8)\n",
    "\n",
    "            feed_dict = {images: batch_images,\n",
    "                         labels: batch_label}\n",
    "            cl = sess.run(tf.argmax(cls, axis=1), feed_dict=feed_dict)\n",
    "            y_true.append(batch_label[0])\n",
    "            y_pred.append(cl[0])\n",
    "            if cl[0] == batch_label[0]:\n",
    "                i += 1\n",
    "            print(\"Epoch: [%4d], real: %d, d: %d\" % (idx, batch_label[0], cl[0]))\n",
    "\n",
    "        print(i/5000)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
